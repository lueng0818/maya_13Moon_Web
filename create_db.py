import sqlite3
import pandas as pd
import os
import glob

DB_NAME = "13moon.db"
DATA_DIR = "data"

def find_file(keyword):
    """æ¨¡ç³Šæœå°‹æª”æ¡ˆ"""
    if not os.path.exists(DATA_DIR): return None
    files = glob.glob(os.path.join(DATA_DIR, "*.csv"))
    for f in files:
        if keyword in os.path.basename(f): return f
    return None

def read_csv_robust(file_path, **kwargs):
    """è¬èƒ½ç·¨ç¢¼è®€å–"""
    encodings = ['utf-8', 'cp950', 'big5', 'utf-8-sig', 'gbk']
    for enc in encodings:
        try:
            return pd.read_csv(file_path, encoding=enc, **kwargs)
        except: continue
    return None

def process_matrix_csv(file_path):
    """è™•ç†çŸ©é™£è¡¨ (é›™å±¤æ¨™é¡Œèˆ‡å»é‡è¤‡æ¬„ä½)"""
    try:
        df = read_csv_robust(file_path, header=[0, 1])
        if df is None: return None
        new_columns = []
        last_top = "Unknown"
        for top, bottom in df.columns:
            if "Unnamed" not in str(top): last_top = str(top).strip()
            clean_bottom = str(bottom).replace('\n', '').strip()
            new_columns.append(f"{last_top}_{clean_bottom}")
        
        # å»é‡è¤‡
        final_cols = []
        counts = {}
        for col in new_columns:
            if col in counts:
                counts[col] += 1
                final_cols.append(f"{col}_{counts[col]}")
            else:
                counts[col] = 1
                final_cols.append(col)
        df.columns = final_cols
        return df
    except: return None

def init_db():
    print(f"ğŸš€ é–‹å§‹å»ºç½®è³‡æ–™åº«: {DB_NAME}...")
    if os.path.exists(DB_NAME): os.remove(DB_NAME)
    conn = sqlite3.connect(DB_NAME)
    
    # 1. è¨ˆç®—èˆ‡åƒç…§è¡¨
    ref_tables = [
        ("kin_start_year", "Kin_Start", 'å¹´ä»½'), 
        ("month_day_accum", "Month_Accum", 'æœˆä»½'), 
        ("kin_basic_info", "Kin_Basic", 'KIN'), 
        ("PSIå°è¨˜å°ç…§è¡¨", "PSI_Bank", 'æœˆæ—¥'), 
        ("å¥³ç¥å°è¨˜", "Goddess_Seal", 'KIN'),
        ("å°æ‡‰ç‘ªé›…ç”Ÿæ—¥", "Calendar_Converter", 'åœ‹æ›†ç”Ÿæ—¥'),
        ("ä¸ƒåƒ¹è·¯å¾‘å°æ‡‰ç¥ˆç¦±æ–‡", "Heptad_Prayer", 'ä¸ƒåƒ¹è·¯å¾‘'),
        ("ç‘ªäºé€±é—œéµå¥", "Maya_Week_Key", 'ç‘ªé›…é€±'),
        ("å…«åº¦éŸ³éš", "Octave_Scale", 'å…«åº¦éŸ³ç¬¦')
    ]

    for keyword, table_name, index_col in ref_tables:
        f = find_file(keyword)
        if f:
            print(f"ğŸ”¹ åŒ¯å…¥ {table_name}: {os.path.basename(f)}")
            df = read_csv_robust(f)
            if df is not None: 
                df.columns = [str(c).strip() for c in df.columns]
                if 'KIN' in df.columns:
                    df['KIN'] = pd.to_numeric(df['KIN'], errors='coerce').fillna(0).astype(int)
                
                df.to_sql(table_name, conn, if_exists="replace", index=False)
                if index_col in df.columns:
                    conn.execute(f"CREATE INDEX IF NOT EXISTS idx_{table_name.lower()} ON {table_name} ({index_col})")

    # 2. æ ¸å¿ƒè³‡æ–™ (çŸ©é™£ã€æ˜“ç¶“ã€å“çˆ¾é‡‘)
    for keyword, table_name in [("å“çˆ¾é‡‘æ›†", "Kin_Data"), ("çŸ©é™£", "Matrix_Data"), ("éŠ€æ²³æ˜“ç¶“", "IChing")]:
        f = find_file(keyword)
        if f:
            print(f"ğŸ”¹ åŒ¯å…¥ {table_name}: {os.path.basename(f)}")
            if keyword == "çŸ©é™£": df = process_matrix_csv(f)
            else: df = read_csv_robust(f)
            
            if df is not None:
                if keyword != "çŸ©é™£": df.columns = [str(c).replace('\n', '').strip() for c in df.columns]
                df.to_sql(table_name, conn, if_exists="replace", index=False)

    # 3. åœ‹ç‹é è¨€æ£‹ç›¤ (NEW)
    f_king = find_file("åœ‹ç‹é è¨€æ£‹ç›¤")
    if f_king:
        print(f"ğŸ”¹ åŒ¯å…¥åœ‹ç‹æ£‹ç›¤: {os.path.basename(f_king)}")
        # æ£‹ç›¤çµæ§‹è¼ƒç‰¹æ®Šï¼Œç›´æ¥åŒ¯å…¥ä¸åšå¤ªå¤šè™•ç†ï¼Œä¾›å‰ç«¯ç›´æ¥é¡¯ç¤º
        df = read_csv_robust(f_king)
        if df is not None: df.to_sql("King_Prophecy", conn, if_exists="replace", index=False)

    # 4. é€šè¨ŠéŒ„ (Users)
    print("ğŸ”¹ å»ºç«‹ Users è¡¨æ ¼...")
    conn.execute("""
        CREATE TABLE IF NOT EXISTS Users (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            å§“å TEXT,
            ç”Ÿæ—¥ TEXT,
            KIN INTEGER,
            ä¸»å°è¨˜ TEXT
        )
    """)
    
    f_user = find_file("é€šè¨ŠéŒ„")
    if f_user:
        df = read_csv_robust(f_user)
        if df is not None:
            try:
                col_map = {'åå­—': 'å§“å', 'Name': 'å§“å'}
                df.rename(columns=col_map, inplace=True)
                required_cols = ['å‡ºç”Ÿå¹´', 'å‡ºç”Ÿæœˆ', 'å‡ºç”Ÿæ—¥']
                if 'å§“å' in df.columns and all(c in df.columns for c in required_cols):
                    for c in required_cols: df[c] = pd.to_numeric(df[c], errors='coerce')
                    df_clean = df.dropna(subset=required_cols).copy()
                    df_clean['ç”Ÿæ—¥'] = df_clean.apply(lambda x: f"{int(x['å‡ºç”Ÿå¹´'])}-{int(x['å‡ºç”Ÿæœˆ'])}-{int(x['å‡ºç”Ÿæ—¥'])}", axis=1)
                    
                    valid_cols = ['å§“å', 'ç”Ÿæ—¥', 'KIN', 'ä¸»å°è¨˜']
                    for vc in valid_cols: 
                        if vc not in df_clean.columns: df_clean[vc] = None
                    df_clean[valid_cols].to_sql("Users", conn, if_exists="append", index=False)
            except: pass

    conn.close()
    print("ğŸ‰ è³‡æ–™åº«å»ºç½®å®Œæˆï¼")

if __name__ == "__main__":
    init_db()
